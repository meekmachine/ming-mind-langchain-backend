# -*- coding: utf-8 -*-
"""convo-topic-emotion-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kQhVwHHvPdpENStQfrzWh3SQlIr4Nl6W
"""

!pip install bertopic
!pip install convokit
!pip install langchain
!pip install transformers
!pip install firebase-admin
!pip install transformers

from bertopic import BERTopic
from convokit import Corpus, download
import pandas as pd
import numpy as np
from langchain.llms import HuggingFaceHub
import os
from transformers import pipeline
import firebase_admin
from firebase_admin import credentials, firestore, initialize_app, storage
from transformers import pipeline
import matplotlib.pyplot as plt

cred = credentials.Certificate("/content/ming-527ed-firebase-adminsdk-z38ui-431c72dad9.json")
firebase_admin.initialize_app(cred)

db = firestore.client()

"""# Loading Corpus"""

corpus = Corpus(filename=download("conversations-gone-awry-corpus"))

conversations_df = corpus.get_conversations_dataframe()
utterances_df = corpus.get_utterances_dataframe()
speakers_df = corpus.get_speakers_dataframe()

utterances_df

class EmotionAnalysis:
    def __init__(self):
        self.classifier = pipeline(task="text-classification", model="SamLowe/roberta-base-go_emotions", top_k=None,device=0)
        self.emotion_dict = {
            'admiration': 'ðŸ˜',
            'amusement': 'ðŸ˜„',
            'anger': 'ðŸ˜¡',
            'annoyance': 'ðŸ˜’',
            'approval': 'ðŸ‘',
            'caring': 'â¤ï¸',
            'confusion': 'ðŸ˜•',
            'curiosity': 'ðŸ¤”',
            'desire': 'ðŸ˜',
            'disappointment': 'ðŸ˜ž',
            'disapproval': 'ðŸ‘Ž',
            'disgust': 'ðŸ¤¢',
            'embarrassment': 'ðŸ˜³',
            'excitement': 'ðŸ˜ƒ',
            'fear': 'ðŸ˜¨',
            'gratitude': 'ðŸ™',
            'grief': 'ðŸ˜¢',
            'joy': 'ðŸ˜',
            'love': 'â¤ï¸',
            'nervousness': 'ðŸ˜¬',
            'neutral': 'ðŸ˜',
            'optimism': 'ðŸ˜Š',
            'pride': 'ðŸ†',
            'realization': 'ðŸ’¡',
            'relief': 'ðŸ˜Œ',
            'remorse': 'ðŸ˜”',
            'sadness': 'ðŸ˜¢',
            'surprise': 'ðŸ˜²'
        }

    def __call__(self, sentences):
        #if sentences == "" or sentences == []:
           # return

        if isinstance(sentences, str):
            sentences = [sentences]

        for i, sentence in enumerate(sentences):
            if len(sentence) > 512:
                sentences[i] = sentence[-512:]


        model_outputs = self.classifier(sentences)
        emotion = model_outputs[0][0]['label']
        if emotion == 'neutral':
            emotion = model_outputs[0][1]['label']
        emoji = self.emotion_dict[emotion]
        return model_outputs, emotion, emoji

classifier = EmotionAnalysis()

def process_conversations(conversations_df, utterances_df, speakers_df):
    # Filter and merge dataframes
    merged_df = pd.merge(utterances_df, speakers_df, left_on='speaker', right_index=True)
    merged_df = pd.merge(merged_df, conversations_df, left_on='conversation_id', right_index=True)

    # Process each conversation

    convo_records = []
    convo_texts = []
    convo_speakers = {}
    convo_speakers_texts = {}
    for convo_id, convo_data in merged_df.groupby('conversation_id'):
        # Calculate total personal attacks and average toxicity
        total_attacks = convo_data['meta.comment_has_personal_attack'].sum()
        avg_toxicity = convo_data['meta.toxicity'].mean()

        # Get conversation start time
        start_time = convo_data['timestamp'].min()

        # Calculate embeddings for the conversation
        convo_text = ' '.join(convo_data['text'].tolist())
        #embeddings = calculate_embeddings(convo_text)

        # Prepare data for Firebase
        convo_record = {
            'convo_id': convo_id,
            'total_personal_attacks': total_attacks,
            'average_toxicity': avg_toxicity,
            'start_time': start_time,
            #'embeddings': embeddings,
            # Include additional metadata as needed
        }

        # Get speakers
        speakers = []
        speaker_texts = {}
        for speaker_id, speaker_data in convo_data.groupby('speaker'):
            speaker_text = ' '.join(speaker_data['text'].tolist())

            speaker_texts[speaker_id] = speaker_text
            a ,speaker_emotion, speaker_emoji = classifier(speaker_text)
            speaker_record = {
                'speaker_id': speaker_id,
                'emotion': speaker_emotion,
                'emoji': speaker_emoji,
                # Include additional metadata as
            }
            speakers.append(speaker_record)

        #convo_speakers.append(convo_speaker)
        convo_speakers_texts[convo_id] =  speaker_texts
        convo_speakers[convo_id] = speakers

        convo_texts.append(convo_text)
        convo_records.append(convo_record)
        # Save to Firebase
        #db.collection('conversations').document(convo_id).set(convo_record)

    print("Processed Convo Texts")
    return convo_records, convo_texts, convo_speakers, convo_speakers_texts

"""meaningfull labels
similarities for the clusters
emotions model to work on classifying speakers on emotions. then turn the emotions



"""

convo_records, convo_texts, convo_speakers, convo_speakers_text  = process_conversations(conversations_df, utterances_df, speakers_df) #.head(1000)

convo_id = convo_records[0]["convo_id"]

merged_df = pd.merge(utterances_df, speakers_df, left_on='speaker', right_index=True)
merged_df = pd.merge(merged_df, conversations_df, left_on='conversation_id', right_index=True)

df = merged_df[merged_df['conversation_id'] == convo_id ].sort_values('timestamp', ascending=True)

df

convo_records[0]["convo_id"]

corpus.get_conversation(convo_records[0]["convo_id"])

corpus.get_conversation("407508250.100949.100949")

convo_speakers['182614870.5054.5054']

convo_speakers_text['204073727.3013.3013']

convo_speakers['211754039.54581.54581']

"""# Do Clustering"""

from sklearn.feature_extraction.text import CountVectorizer
vectorizer_model = CountVectorizer(stop_words="english")
topic_model = BERTopic(embedding_model="all-MiniLM-L6-v2",vectorizer_model=vectorizer_model,nr_topics=100)#calculate_probabilities=True)

topics, probs = topic_model.fit_transform(convo_texts)
new_topics = topic_model.reduce_outliers(convo_texts, topics, strategy="c-tf-idf")
new_topics = topic_model.reduce_outliers(convo_texts, topics, strategy="distributions")
new_topics = topic_model.reduce_outliers(convo_texts, topics, strategy="embeddings")

topic_model.get_topic_info()

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt

distance_matrix = cosine_similarity(np.array(topic_model.topic_embeddings_)[:, :])
labels = (topic_model.get_topic_info().sort_values("Topic", ascending=True).Name)[1:]
plt.imshow(distance_matrix)

topic_list = topic_model.topic_labels_

topic_list.keys()

"""# Set Up Data for Firebase"""

label_records = []

for i in topic_list.keys():
  label_record = {}
  label_record['topic'] = i
  topic_words = [token[0] for token in topic_model.get_topic(i)]
  topic_name = topic_words[0]
  for topic_word in topic_words[1:4]:
    topic_name += "_" + topic_word
  label_record['topic_name'] = topic_name

  sim = distance_matrix[i+1]
  for j in range(len(sim)):
    label_record[str(j-1)] = float(sim[j])

  label_records.append(label_record)

for i in range(len(convo_records)):
    convo_records[i]['topic'] = topics[i]

    topic_words = [token[0] for token in topic_model.get_topic(topics[i])]
    topic_name = topic_words[0]
    for topic_word in [token[0] for token in topic_model.get_topic(topics[i])][1:4]:
      topic_name += "_" + topic_word

    convo_records[i]['topic_name'] = topic_name
    convo_records[i]['probs_per_topic'] = probs[i]

label_records[6]

convo_records[0]

"""## Filter out conversations by labels to make it easier to send to firebase"""

sorted_conv = {}
for record in label_records:
  for conv in convo_records:
    if record['topic'] == conv['topic']:
      if record['topic'] not in sorted_conv.keys():
        sorted_conv[record['topic']] = [conv]
      else:
        sorted_conv[record['topic']].append(conv)

sorted_conv[-1][0].keys()

import math
for i, record in enumerate(label_records):
  sum = 0
  count = 0
  for convo_record in sorted_conv[i-1]:
    if math.isnan(convo_record['average_toxicity']):
      continue
    count += 1
    sum += convo_record['average_toxicity']
  if count == 0:
    avg = 0
  else:
    avg = sum/count
  label_records[i]['average_toxicity'] = avg

"""## Save to Firebase"""

for i, label_record in enumerate(label_records):
    topic = label_record['topic']
    topic_id = str(i)
    db.collection('topics1').document(topic_id).set(label_record)
    for j, convo_record in enumerate(sorted_conv[topic]):
        convo_id = convo_record['convo_id']
        db.collection('topics1').document(topic_id).collection('convosations').document(convo_id).set(convo_record)
        for speaker_record in convo_speakers[convo_id]:
            speaker_id = speaker_record['speaker_id']
            db.collection('topics1').document(topic_id).collection('convosations').document(convo_id).collection('speakers').document(speaker_id).set(speaker_record)
print('done')

for i, record in enumerate(convo_records):
    convo_id = record['convo_id']
    db.collection('convo_topics').document(convo_id).set(record)